import streamlit as st
import os
import time
import requests
from dotenv import load_dotenv
from openai import OpenAI

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="AI æ™ºèƒ½æŠ¥é”™ä¿®å¤åŠ©æ‰‹",
    page_icon="ğŸš‘",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- æ ¸å¿ƒé€»è¾‘å‡½æ•° (å¤ç”¨åŸè„šæœ¬é€»è¾‘) ---

def clean_error_log_with_slm(raw_log, model_name):
    """è°ƒç”¨æœ¬åœ° Ollama æ¸…æ´—æ—¥å¿—"""
    url = "http://localhost:11434/api/generate"
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæŠ¥é”™æ—¥å¿—æ¸…æ´—å·¥å…·ã€‚è¯·ä»ä¸‹é¢çš„æ‚ä¹±æ—¥å¿—ä¸­æå–ï¼š
    1. é”™è¯¯ç±»å‹ (Error Type)
    2. å¯¼è‡´é”™è¯¯çš„ç”¨æˆ·ä»£ç è¡Œå· (User Code Line)
    3. æ ¸å¿ƒæŠ¥é”™ä¿¡æ¯ (Core Message)
    
    å¿½ç•¥æ‰€æœ‰ç³»ç»Ÿåº“(System Libs)å’Œæ¡†æ¶å±‚(Framework)çš„å †æ ˆä¿¡æ¯ã€‚
    åªè¾“å‡ºçº¯æ–‡æœ¬æ‘˜è¦ï¼Œä¸è¦Markdownã€‚
    
    æ—¥å¿—å†…å®¹ï¼š
    {raw_log}
    """
    
    data = {
        "model": model_name,
        "prompt": prompt,
        "stream": False,
    }

    try:
        response = requests.post(url, json=data, timeout=30)
        response.raise_for_status()
        return response.json().get("response", "âŒ æœ¬åœ°æ¸…æ´—å¤±è´¥: è¿”å›ç¼ºå°‘ response å­—æ®µ")
    except requests.RequestException as exc:
        return f"âŒ æœ¬åœ°è¿æ¥å¤±è´¥ (è¯·æ£€æŸ¥ Ollama æ˜¯å¦è¿è¡Œ): {exc}"
    except Exception as e:
        return f"âŒ æœªçŸ¥é”™è¯¯: {str(e)}"

def ask_expert_llm(user_code, error_summary, api_key, model="deepseek-chat"):
    """è°ƒç”¨äº‘ç«¯ DeepSeek åˆ†æ"""
    if not api_key:
        return "âš ï¸ æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·åœ¨å·¦ä¾§ä¾§è¾¹æ è¾“å…¥æˆ–é…ç½® .env æ–‡ä»¶ã€‚"

    prompt = f"""
    æˆ‘é‡åˆ°ä¸€ä¸ªæŠ¥é”™ï¼Œè¯·å¸®æˆ‘ä¿®å¤ã€‚

    ã€æˆ‘çš„ä»£ç ã€‘ï¼š
    {user_code}

    ã€æŠ¥é”™å…³é”®ä¿¡æ¯ã€‘(ç”±æœ¬åœ°åŠ©æ‰‹æå–)ï¼š
    {error_summary}

    è¯·åˆ†æåŸå› å¹¶ç»™å‡ºä¿®æ”¹åçš„ä»£ç ã€‚è¯·ä½¿ç”¨ Markdown æ ¼å¼è¾“å‡ºã€‚
    """

    client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1000,
            temperature=0.3,
        )
        return resp.choices[0].message.content
    except Exception as exc:
        return f"âŒ äº‘ç«¯è°ƒç”¨å¤±è´¥: {exc}"

# --- ç•Œé¢ UI æ„å»º ---

# 1. ä¾§è¾¹æ ï¼šè®¾ç½®ä¸ä»‹ç»
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    
    # å°è¯•è‡ªåŠ¨åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    env_key = os.getenv("DEEPSEEK_API_KEY", "")
    
    api_key = st.text_input("DeepSeek API Key", value=env_key, type="password", help="å¦‚æœæ²¡æœ‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·åœ¨æ­¤å¤„è¾“å…¥")
    ollama_model = st.text_input("æœ¬åœ° Ollama æ¨¡å‹", value="qwen2.5-coder:1.5b", help="è¯·ç¡®ä¿æœ¬åœ° Ollama å·²å®‰è£…æ­¤æ¨¡å‹")
    
    st.markdown("---")
    st.markdown("""
    ### ğŸ“– å…³äºæœ¬å·¥å…·
    è¿™æ˜¯ä¸€ä¸ª **éšç§ä¼˜å…ˆ** çš„æŠ¥é”™ä¿®å¤åŠ©æ‰‹ã€‚
    
    **å·¥ä½œåŸç†ï¼š**
    1. **æœ¬åœ°å°æ¨¡å‹** (Ollama) é¦–å…ˆè¿è¡Œï¼Œæ¸…æ´—å†—é•¿çš„æŠ¥é”™å †æ ˆï¼Œæå–å…³é”®ä¿¡æ¯ã€‚
    2. **äº‘ç«¯å¤§æ¨¡å‹** (DeepSeek) æ¥æ”¶ç²¾ç®€åçš„ä¿¡æ¯å’Œä»£ç ï¼Œæä¾›ä¿®å¤æ–¹æ¡ˆã€‚
    
    è¿™æ ·åšæ—¢ä¿æŠ¤äº†éšç§ï¼ŒåˆèŠ‚çœäº† Tokenã€‚
    """)

# 2. ä¸»é¡µé¢ï¼šæ ‡é¢˜ä¸è¾“å…¥
st.title("ğŸš‘ AI æ™ºèƒ½æŠ¥é”™ä¿®å¤åŠ©æ‰‹")
st.markdown("é‡åˆ° Bug äº†ï¼Ÿåˆ«æ‹…å¿ƒã€‚ç²˜è´´ä½ çš„ä»£ç å’ŒæŠ¥é”™æ—¥å¿—ï¼ŒAI ä¼šå¸®ä½ æ‰¾å‡ºé—®é¢˜æ‰€åœ¨ã€‚")

col1, col2 = st.columns(2)

with col1:
    st.subheader("1ï¸âƒ£ ä½ çš„ä»£ç  (Python)")
    user_code = st.text_area("ç²˜è´´ç›¸å…³ä»£ç ç‰‡æ®µ", height=300, placeholder="def my_function():...")

with col2:
    st.subheader("2ï¸âƒ£ æŠ¥é”™æ—¥å¿— (Traceback)")
    error_log = st.text_area("ç²˜è´´å®Œæ•´æŠ¥é”™ä¿¡æ¯", height=300, placeholder="Traceback (most recent call last)...")

# 3. æ‰§è¡ŒæŒ‰é’®ä¸ç»“æœå±•ç¤º
if st.button("ğŸš€ å¼€å§‹è¯Šæ–­", type="primary", use_container_width=True):
    if not user_code or not error_log:
        st.warning("âš ï¸ è¯·åŒæ—¶è¾“å…¥ä»£ç å’ŒæŠ¥é”™æ—¥å¿—ã€‚")
    else:
        # ä½¿ç”¨ st.status åˆ›å»ºä¸€ä¸ªåŠ¨æ€çš„çŠ¶æ€å®¹å™¨
        with st.status("æ­£åœ¨è¿›è¡Œ AI è¯Šæ–­...", expanded=True) as status:
            
            # ç¬¬ä¸€æ­¥ï¼šæœ¬åœ°æ¸…æ´—
            st.write("ğŸ” [Step 1] æ­£åœ¨å”¤é†’æœ¬åœ°å°æ¨¡å‹ (Ollama) æ¸…æ´—æ—¥å¿—...")
            start_time = time.time()
            clean_log = clean_error_log_with_slm(error_log, ollama_model)
            
            if "âŒ" in clean_log:
                status.update(label="æœ¬åœ°æ¸…æ´—å¤±è´¥", state="error")
                st.error(clean_log)
                st.stop()
            else:
                st.write(f"âœ… æ—¥å¿—æ¸…æ´—å®Œæˆ (è€—æ—¶ {time.time()-start_time:.2f}s)")
                # åœ¨è¿™é‡Œå±•ç¤ºæ¸…æ´—åçš„ç»“æœç»™ç”¨æˆ·çœ‹ï¼ˆå¢åŠ é€æ˜åº¦ï¼‰
                with st.expander("ğŸ‘€ ç‚¹å‡»æŸ¥çœ‹æ¸…æ´—åçš„å…³é”®æŠ¥é”™ä¿¡æ¯"):
                    st.code(clean_log, language="text")

            # ç¬¬äºŒæ­¥ï¼šäº‘ç«¯åˆ†æ
            st.write("ğŸ§  [Step 2] æ­£åœ¨å‘é€ç»™äº‘ç«¯ä¸“å®¶ (DeepSeek)...")
            solution = ask_expert_llm(user_code, clean_log, api_key)
            
            if "âŒ" in solution or "âš ï¸" in solution:
                status.update(label="äº‘ç«¯åˆ†æé‡åˆ°é—®é¢˜", state="error")
                st.error(solution)
            else:
                status.update(label="âœ… è¯Šæ–­å®Œæˆï¼", state="complete")
                
                st.divider()
                st.subheader("ğŸ’¡ ä¿®å¤å»ºè®®")
                st.markdown(solution)